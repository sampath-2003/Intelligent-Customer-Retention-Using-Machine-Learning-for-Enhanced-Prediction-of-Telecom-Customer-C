# -*- coding: utf-8 -*-
"""churn project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TakrBYN_Ak3r3bGEMhKFAGI51zvLCa28

2nd task started
"""

# Commented out IPython magic to ensure Python compatibility.

import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import sklearn 
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import imblearn
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score,classification_report, confusion_matrix, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

df = pd.read_csv('/content/Churn_Modelling (2).csv')
df

df.info()

df.isnull().any()

df.isnull().sum()

le=LabelEncoder()
df["RowNumber"]=le.fit_transform(df["RowNumber"])
df["CustomerId"]=le.fit_transform(df["CustomerId"])
df["Surname"]=le.fit_transform(df["Surname"])
df["CreditScore"]=le.fit_transform(df["CreditScore"])
df["Geography"]=le.fit_transform(df["Geography"])
df["Gender"]=le.fit_transform(df["Gender"])
df["Age"]=le.fit_transform(df["Age"])
df["Tenure"]=le.fit_transform(df["Tenure"])
df["Balance"]=le.fit_transform(df["Balance"])
df["NumOfProducts"]=le.fit_transform(df["NumOfProducts"])
df["HasCrCard"]=le.fit_transform(df["HasCrCard"])
df["IsActiveMember"]=le.fit_transform(df["IsActiveMember"])
df["EstimatedSalary"]=le.fit_transform(df["EstimatedSalary"])
df["Exited"]=le.fit_transform(df["Exited"])

df

x=df.iloc[:,0:13].values
y=df.iloc[:,13:14].values

x,y

one=OneHotEncoder()
a=one.fit_transform(x[:,1:2]).toarray()
b=one.fit_transform(x[:,2:3]).toarray()
c=one.fit_transform(x[:,3:4]).toarray()
d=one.fit_transform(x[:,4:5]).toarray()
e=one.fit_transform(x[:,5:6]).toarray()
f=one.fit_transform(x[:,7:8]).toarray()
g=one.fit_transform(x[:,8:9]).toarray()
h=one.fit_transform(x[:,9:10]).toarray()
i=one.fit_transform(x[:,10:11]).toarray()
j=one.fit_transform(x[:,11:12]).toarray()
k=one.fit_transform(x[:,12:13]).toarray()
x=np.delete(x,[1,2,3,4,5,7,8,9,10,11,12],axis=1)
x=np.concatenate((a,b,c,d,e,f,g,h,i,k,x),axis=1)

x

smt=SMOTE()
x_resample,y_resample=smt.fit_resample(x,y)

x_resample,y_resample

x.shape,x_resample.shape

y.shape,y_resample.shape

"""# 3rd task started"""

df.describe()

plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
sns.distplot(df["Tenure"])
plt.subplot(1,2,2)
sns.distplot(df["CreditScore"])

sns.countplot(data=df, x="Gender")

sns.barplot(data=df, x="Tenure", y="Balance", color="b")
plt.title("Tenure VS Balance")

sns.heatmap(df.corr(),annot=True)

sns.pairplot(data=df, markers=["^","v"], palette="inferno")

x = df.drop(columns=["Exited"])
y = df["Balance"]
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=0)

sc=StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.fit_transform(x_test)

"""# 4th task started"""

x_train.shape

def logreg(x_train,x_test,y_train,y_test):
  lr=LogisticRegression(random_state=0)
  lr.fit(x_train,y_train)
  y_lr_tr = lr.predict(x_train)

#importing and building the Decision tree model
def logreg(x_train,x_test,y_train,y_test):
  lr=LogisticRegression(random_state=0)
  lr.fit(x_train,y_train)
  y_lr_tr = lr.predict(x_train)
  print(accuracy_score(y_lr_tr,y_train))
  yPred_lr = lr.predict(x_test)
  print(accuracy_score(yPred_lr,y_test))
  print("***Logistic Regression***")
  print("Confusion_Matrix")
  print(confusion_matrix(y_test,yPred_lr))
  print("Classification Report")
  print(classification_report(y_test,yPred_lr))

logreg(x_train, x_test, y_train, y_test)

#importing and building the Decision tree model
def decisionTree(x_train,x_test,y_train,y_test):
  dtc = DecisionTreeClassifier(criterion="entropy",random_state=0)
  dtc.fit(x_train,y_train)
  y_dt_tr = dtc.predict(x_train)
  print(accuracy_score(y_dt_tr,y_train))
  yPred_dt = dtc.predict(x_test)
  print(accuracy_score(yPred_dt,y_test))
  print("***Decision Tree***")
  print("Confusion_Matrix")
  print(confusion_matrix(y_test,yPred_dt))
  print("Classification Report")
  print(classification_report(y_test,yPred_dt))

decisionTree(x_train,x_test,y_train,y_test)

#importing and building the random forest model
def RandomForest(x_train,x_test,y_train,y_test):
  rf = RandomForestClassifier(criterion="entropy",n_estimators=10,random_state=0)
  rf.fit(x_train,y_train)
  y_rf_tr = rf.predict(x_train)
  print(accuracy_score(y_rf_tr,y_train))
  yPred_rf = rf.predict(x_test)
  print(accuracy_score(yPred_rf,y_test))
  print("***Random forest***")
  print("Confusion_Matrix")
  print(confusion_matrix(y_test,yPred_rf))
  print("Classification Report")
  print(classification_report(y_test,yPred_rf))

RandomForest(x_train,x_test,y_train,y_test)

#importing and building the KNN model
def KNN(x_train,x_test,y_train,y_test):
    Knn = KNeighborsClassifier()
    Knn.fit(x_train,y_train)
    y_Knn_tr = Knn.predict(x_train)
    print(accuracy_score(y_Knn_tr,y_train))
    yPred_Knn = Knn.predict(x_test)
    print(accuracy_score(yPred_Knn,y_test))
    print("***KNN***")
    print("Confusion_Matrix")
    print(confusion_matrix(y_test,yPred_Knn))
    print("Classification Report")
    print(classification_report(y_test,yPred_Knn))

KNN(x_train,x_test,y_train,y_test)

#importing and building the ramdom forest mode1
def svm(x_train, x_test, y_train, y_test):
    svm = SVC(kernel="linear")
    svm.fit(x_train, y_train)
    y_svm_tr = svm.predict(x_train)
    print(accuracy_score(y_svm_tr, y_train))
    yPred_svm = svm.predict(x_test)
    print(accuracy_score(yPred_svm, y_test))
    print("***Support Vector Machine***")
    print("Confusion_Matrix")
    print(confusion_matrix(y_test, yPred_svm))
    print("Classification Report")
    print(classification_report(y_test, yPred_svm))
    
# Call the function with appropriate arguments

svm(x_train, x_test, y_train, y_test)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# load data
x = np.random.rand(200, 40)  # example input data with 200 samples and 40 features
y = np.random.randint(0, 2, size=(200, 1))  # example target labels (binary)

# split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)

# define the model
num_features = x_train.shape[1]
classifier = Sequential()
classifier.add(Dense(units=32, activation='relu', input_shape=(num_features,)))
classifier.add(Dense(units=16, activation='relu'))
classifier.add(Dense(units=1, activation='sigmoid'))

# compile the model
optimizer = Adam(learning_rate=0.001)
classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# train the model
history = classifier.fit(x_train, y_train, batch_size=32, validation_split=0.2, epochs=50)

model_history = classifier.fit(x_train, y_train, batch_size=10, validation_split=0.33, epochs=200)

ann_pred = classifier.predict(x_test)
ann_pred = (ann_pred > 0.5)
print(ann_pred)

accuracy = accuracy_score(y_test, ann_pred)
conf_matrix = confusion_matrix(y_test, ann_pred)
class_report = classification_report(y_test, ann_pred)

print("***ANN Model***")
print("Accuracy Score:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)





# create a StandardScaler object and fit it on the training data
sc = StandardScaler()
sc.fit(x_train)

# transform the training and test data with the scaler object
x_train_scaled = sc.transform(x_train)
x_test_scaled = sc.transform(x_test)

# create and train the logistic regression model
lr = LogisticRegression(random_state=0)
lr.fit(x_train_scaled, y_train)

# predict on test data and evaluate the model
lr_pred = lr.predict(x_test_scaled)
print("***Logistic Regression Model***")
print("Accuracy Score")
print(accuracy_score(lr_pred, y_test))
print("Confusion Matrix")
print(confusion_matrix(lr_pred, y_test))
print("Classification Report")
print(classification_report(lr_pred, y_test))

# predicting on randon input
lr_pred_own = lr.predict(sc.transform([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]]))
print("Predicted output is:", lr_pred_own)



#testing on random input values
dt = DecisionTreeClassifier(random_state=0)
dt.fit(x_train, y_train)

# predict on test data and evaluate the model
dt_pred = dt.predict(x_test)
print("***Decision Tree Classifier Model***")
print("Accuracy Score")
print(accuracy_score(dt_pred, y_test))
print("Confusion Matrix")
print(confusion_matrix(dt_pred, y_test))
print("Classification Report")
print(classification_report(dt_pred, y_test))

# predicting on randon input
dt_pred_own = dt.predict([[0,0,1,1,0,0,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,0,1,0,1,0,0,1,1,0,0,456,1,0,3245,4567]])
print("Predicted output is:", dt_pred_own)

x_train_scaled = sc.transform(x_train)
x_test_scaled = sc.transform(x_test)

# create and train the random forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, criterion='gini', random_state=0)
rf_classifier.fit(x_train_scaled, y_train)

# predict on test data and evaluate the model
rf_pred = rf_classifier.predict(x_test_scaled)
print("***Random Forest Classifier Model***")
print("Accuracy Score")
print(accuracy_score(rf_pred, y_test))
print("Confusion Matrix")
print(confusion_matrix(rf_pred, y_test))
print("Classification Report")
print(classification_report(rf_pred, y_test))

# predict on a random input
random_input = sc.transform([[0]*40])
print("Predicting on random input")
rf_pred_own = rf_classifier.predict(random_input)
print("Output is:", rf_pred_own)

x_train_scaled = sc.transform(x_train)
x_test_scaled = sc.transform(x_test)

# create and train the SVM classifier
svm_classifier = SVC(kernel='linear', random_state=0)
svm_classifier.fit(x_train_scaled, y_train)

# predict on test data and evaluate the model
svm_pred = svm_classifier.predict(x_test_scaled)
print("***SVM Classifier Model***")
print("Accuracy Score")
print(accuracy_score(svm_pred, y_test))
print("Confusion Matrix")
print(confusion_matrix(svm_pred, y_test))
print("Classification Report")
print(classification_report(svm_pred, y_test))

# predict on a random input
random_input = sc.transform([[0]*40])
print("Predicting on random input")
svm_pred_own = svm_classifier.predict(random_input)
print("Output is:", svm_pred_own)

knn_classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
knn_classifier.fit(x_train_scaled, y_train)

# predict on test data and evaluate the model
knn_pred = knn_classifier.predict(x_test_scaled)
print("***KNN Classifier Model***")
print("Accuracy Score")
print(accuracy_score(knn_pred, y_test))
print("Confusion Matrix")
print(confusion_matrix(knn_pred, y_test))
print("Classification Report")
print(classification_report(knn_pred, y_test))

# predict on a random input
random_input = sc.transform([[0]*40])
print("Predicting on random input")
knn_pred_own = knn_classifier.predict(random_input)
print("Output is:", knn_pred_own)

classifier = Sequential()
classifier.add(Dense(units=30, activation='relu', input_dim=40))
classifier.add(Dense(units=30, activation='relu'))
classifier.add(Dense(units=1, activation='sigmoid'))

# compile the model
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# train the model
model_history = classifier.fit(x_train_scaled, y_train, batch_size=10, validation_split=0.33, epochs=200)

# predict on test data and evaluate the model
ann_pred = classifier.predict(x_test_scaled)
ann_pred = (ann_pred>0.5)
print("***ANN Classifier Model***")
print("Accuracy Score")
print(accuracy_score(ann_pred, y_test))
print("Confusion Matrix")
print(confusion_matrix(ann_pred, y_test))
print("Classification Report")
print(classification_report(ann_pred, y_test))

# predict on a random input
random_input = sc.transform([[0]*40])
print("Predicting on random input")
ann_pred_own = classifier.predict(random_input)
ann_pred_own = (ann_pred_own>0.5)
print("Output is:", ann_pred_own)

# Random Forest Classifier without hyperparameter tuning
rfc = RandomForestClassifier(random_state=42)
rfc.fit(x_train, y_train)
rfc_pred = rfc.predict(x_test)

print("***RFC Model without Hyperparameter Tuning***")
print("Accuracy:", accuracy_score(y_test, rfc_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, rfc_pred))
print("Classification Report:\n", classification_report(y_test, rfc_pred))

# Random Forest Classifier with hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_leaf': [1, 2, 4]
}

rfc_tuned = RandomForestClassifier(random_state=42)
rfc_cv = GridSearchCV(rfc_tuned, param_grid, cv=5)
rfc_cv.fit(x_train, y_train)
rfc_tuned_pred = rfc_cv.predict(x_test)

print("***RFC Model with Hyperparameter Tuning***")
print("Best Parameters:", rfc_cv.best_params_)
print("Accuracy:", accuracy_score(y_test, rfc_tuned_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, rfc_tuned_pred))
print("Classification Report:\n", classification_report(y_test, rfc_tuned_pred))

import pickle
pickle.dump(rfc,open("churn.pkl","wb"))